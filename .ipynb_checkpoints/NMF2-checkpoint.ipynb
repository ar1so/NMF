{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5be64066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn import decomposition\n",
    "from scipy.stats import norm\n",
    "from opnmf import model, logging\n",
    "import opnmf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "921bfce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifting_values_min(data) : \n",
    "\n",
    "    for col in data.columns:\n",
    "    \n",
    "        # Get column min \n",
    "        min_val = data[col].min()\n",
    "        \n",
    "        # Shift column by its min value\n",
    "        data[col] = data[col] + abs(min_val)\n",
    "        #After checking all of the columns were 0 in the min value\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68e3d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(data):\n",
    "    \n",
    "    # Create StandardScaler and fit \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data)\n",
    "\n",
    "    # Transform data\n",
    "    normalized_data = scaler.transform(data)  \n",
    "    normalized_data = pd.DataFrame(normalized_data, columns=data.columns)\n",
    "    \n",
    "    #Assay the normalizide was conducted or not\n",
    "    if normalized_data[normalized_data < 0].sum().sum() == 0  : \n",
    "        data_array = normalized_data.to_numpy()\n",
    "        \n",
    "        return data_array\n",
    "    \n",
    "    else : \n",
    "        normalized_data = shifting_values_min(normalized_data)\n",
    "        data_array = normalized_data.to_numpy()\n",
    "        \n",
    "        return data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac05db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stability_analysis(data_array, n_components_range, num_perturbations = 10, init = 'nndsvd', tolerance = 0.00001, random_state=42):\n",
    "    \"\"\"\n",
    "    Performs stability analysis for OPNMF by training models on perturbed data and evaluating component similarity.\n",
    "\n",
    "    Args:\n",
    "      data_array: The original data matrix.\n",
    "      n_components_range: A range of values for the number of components (n_components) to evaluate.\n",
    "      num_perturbations: The number of perturbed data matrices to generate.\n",
    "      init: The initialization method for OPNMF (default: 'nndsvd').\n",
    "      tolerance: The tolerance parameter for OPNMF (default: 0.0001).\n",
    "\n",
    "    Returns:\n",
    "      optimal_n_components: The optimal number of components based on stability analysis.\n",
    "      stability_coefficients: A dictionary mapping each number of components to its stability coefficient.\n",
    "      reconstruction_errors: A dictionary mapping each number of components to its reconstruction error.\n",
    "    \"\"\"\n",
    "    if random_state:\n",
    "        random.seed(random_state)\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    stability_coefficients = {}\n",
    "    reconstruction_errors = {}\n",
    "\n",
    "    # Train OPNMF on original data to get original components\n",
    "    estimator = opnmf.model.OPNMF(n_components=max(n_components_range), init=init, tol=tolerance)\n",
    "    estimator.fit(data_array)\n",
    "    original_components = estimator.components_\n",
    "\n",
    "    for n_components in n_components_range:\n",
    "        similarities = []\n",
    "\n",
    "        for _ in range(num_perturbations):\n",
    "            # Generate perturbed data\n",
    "            perturbed_data = data_array + norm(0, 0.01).rvs(size=data_array.shape)\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(perturbed_data)\n",
    "\n",
    "            # Transform data\n",
    "            normalized_data = scaler.transform(perturbed_data) \n",
    "            normalized_data = pd.DataFrame(normalized_data, columns=pd.DataFrame(data_array).columns)\n",
    "            \n",
    "            for col in normalized_data.columns:\n",
    "                # Get column min \n",
    "                min_val = normalized_data[col].min()\n",
    "\n",
    "                # Shift column by its min value\n",
    "                normalized_data[col] = normalized_data[col] + abs(min_val)\n",
    "\n",
    "            perturbed_data_array = normalized_data.to_numpy()\n",
    "\n",
    "            # Train OPNMF model on perturbed data\n",
    "            estimator = opnmf.model.OPNMF(n_components=n_components, init=init, tol=tolerance)\n",
    "            W = estimator.fit_transform(perturbed_data_array)\n",
    "            H = estimator.components_\n",
    "\n",
    "            # Compute similarity to original components\n",
    "            similarity = compute_similarity(H, original_components)\n",
    "            similarities.append(similarity)\n",
    "\n",
    "        # Average similarities for this n_components\n",
    "        average_similarity = np.mean(similarities)\n",
    "        stability_coefficients[n_components] = average_similarity\n",
    "\n",
    "        # Reconstruction error for this n_components\n",
    "        reconstruction_error = np.linalg.norm(perturbed_data_array - np.dot(W, H), ord='fro')\n",
    "        reconstruction_errors[n_components] = reconstruction_error\n",
    "\n",
    "    # Find optimal n_components based on highest stability coefficient\n",
    "    optimal_n_components = max(stability_coefficients, key=stability_coefficients.get)\n",
    "\n",
    "    return optimal_n_components, stability_coefficients, reconstruction_errors\n",
    "\n",
    "def compute_similarity(components1, components2, similarity_measure='cosine'):\n",
    "    \"\"\"\n",
    "    Calculates the similarity between two sets of OPNMF components.\n",
    "\n",
    "    Args:\n",
    "      components1: A matrix of components (H) from the first OPNMF model.\n",
    "      components2: A matrix of components (H) from the second OPNMF model.\n",
    "      similarity_measure: The similarity measure to use ('cosine' or 'frobenius').\n",
    "\n",
    "    Returns:\n",
    "      similarity: The similarity score between the two sets of components.\n",
    "    \"\"\"\n",
    "\n",
    "    if similarity_measure == 'cosine':\n",
    "        similarity = np.mean(np.dot(components1, components2.T))\n",
    "    elif similarity_measure == 'frobenius':\n",
    "        similarity = np.linalg.norm(components1 - components2, ord='fro')\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid similarity measure: {similarity_measure}\")\n",
    "\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91a128f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stability_plot(stabilities, errors):\n",
    "    # Extract data\n",
    "    n_components = list(stabilities.keys()) \n",
    "    stability_values = list(stabilities.values())\n",
    "    error_values = list(errors.values())\n",
    "\n",
    "    # Plot \n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('n_components')\n",
    "    ax1.set_ylabel('Stability', color=color) \n",
    "    ax1.plot(n_components, stability_values, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()  \n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Reconstruction Error', color=color)  \n",
    "    ax2.plot(n_components, error_values, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.invert_yaxis()   \n",
    "\n",
    "    fig.tight_layout()  \n",
    "    plt.show()\n",
    "    fig.savefig('stability_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5346a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Reading\n",
    "merge_file = pd.read_csv(\"merge.csv\")\n",
    "df = merge_file.copy()\n",
    "column_get = df.columns[1:-1]\n",
    "df_2 = df[column_get]\n",
    "df = df_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02318636",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = normalization(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d75b27a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.15521502, 3.51111203, 4.04718404, ..., 1.97413043, 2.90267839,\n",
       "        1.65037602],\n",
       "       [4.02217214, 4.21959358, 4.61608765, ..., 1.47967261, 1.99999639,\n",
       "        0.97011588],\n",
       "       [3.14271957, 1.57344163, 3.17869491, ..., 2.94909204, 2.66087498,\n",
       "        2.97751256],\n",
       "       ...,\n",
       "       [4.11509286, 4.41202888, 2.48192696, ..., 2.86597459, 2.0363084 ,\n",
       "        2.31752112],\n",
       "       [3.69933512, 0.99147465, 3.48878136, ..., 1.60695758, 2.81276293,\n",
       "        2.33209339],\n",
       "       [4.10077989, 2.96576768, 1.48680891, ..., 3.0903917 , 2.11232154,\n",
       "        3.31717897]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a62eb7cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,276) (16,276) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9k/k1xv6f3d697_1t1nm64y15zm0000gn/T/ipykernel_18221/685116092.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_perturbations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m  \u001b[0;31m# or any other number of perturbations you want to perform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moptimal_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstability_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_perturbations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optimal number of components:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/9k/k1xv6f3d697_1t1nm64y15zm0000gn/T/ipykernel_18221/376784615.py\u001b[0m in \u001b[0;36mstability_analysis\u001b[0;34m(data_array, n_components_range, num_perturbations, init, tolerance, random_state)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m# Compute similarity to original components\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/9k/k1xv6f3d697_1t1nm64y15zm0000gn/T/ipykernel_18221/376784615.py\u001b[0m in \u001b[0;36mcompute_similarity\u001b[0;34m(components1, components2, similarity_measure)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponents1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponents2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msimilarity_measure\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'frobenius'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponents1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcomponents2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid similarity measure: {similarity_measure}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,276) (16,276) "
     ]
    }
   ],
   "source": [
    "data_array = data_array # your data array\n",
    "n_components_range = (2,4,6,8,10,12,14,16)  # or any other range you want to test\n",
    "num_perturbations = 10  # or any other number of perturbations you want to perform\n",
    "\n",
    "optimal_n, stabilities, errors = stability_analysis(data_array, n_components_range, num_perturbations)\n",
    "\n",
    "print(\"Optimal number of components:\", optimal_n)\n",
    "print(\"Stability coefficients:\", stabilities)\n",
    "print(\"Reconstruction errors:\", errors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
